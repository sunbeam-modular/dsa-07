	- one problem may has many solutions/algorithms.
	- e.g. sorting: to arrange elements in a collection/list (i.e. array or linked
	list) of elements either in an ascending order or in a descending order.
	 	A1 -- selection sort
	 	A2 -- bubble sort
	 	A3 -- insertion sort
	 	A4 -- quick sort
	 	A5 -- merge sort
	 	A6 -- heap sort
	 	A7 -- radix sort
	 	A8 -- shell sort
	 	etc...

	 - when one problem has many solutions/algorithms one need to select an efficient
	 solution/algorithm out of them.
	 - to decide an efficiency of an algorithms we need to do their analysis.
	 - analysis of an algorithm: it is a work of determining how much time i.e.
	 "computer time" and space i.e. "computer memory" an algorithms needs to complete
	 its execution.
	 - there are two measures of an analysis of an algo:
	 1. time complexity: time complexity of an algorithm is the amount computer time it
	 needs to run to completion.
	 
	 2. space complexity: space complexity of an algorithm is the amount computer
	 memory/space it needs to run to completion.
	 
	 * space complexity (space required for any function/program/algorithm):
	 = code space ( i.e. instructions resides in a main memory )
	 + data space (i.e. space required for simple variables, constants and instance
	  variables) 
	 + stack space (applicable only for recursive algorithms): it contains space
	 required for formal parameters, local variables, return addr etc... i.e. function
	 activation records of the called functions.
	 
	+ space complexity has two components:
	1. fixed component: code space + data space (space required for simple variables
	and constants).
	
	2. variable component: instance characteristics i.e. input size + stack space
	(applicable in only case of recursive approach).
	
	
	* time complexity: 
	time complexity = compilation time + execution time
	- compilation time is a fixed component, whereas execution time is a variable
	component.
	- execution time is depends on instance characteristics i.e. input size
	- as an execution time is not only depends on instance chars, it also depends on
	some external factors/environment like type of machine, no. of processes running
	in the system, and hence time complexity cannot be calculated exactly by using
	this approach, therefore another method can be used for calculating time and space
	complexities reffered as "asymptotic analysis".
	
	+ asymptotic analysis: it is a "mathematical" way to calculate time complexity and
	space complexity of an algorithm without implementing it in any programming
	language.
	- to do asymptotic analysis "basic operation" in an algo can be focused.
	e.g. in searching and sorting algo's comparison is the basic operation, and hence
	analysis of such algo's can be done on the basis of no. of comparisons in it.
	
	+ best case time complexity: when an algo takes minimum amount of time to complete
	its execution then it is reffered as best case time complexity.
	
	+ worst case time complexity: when an algo takes maximum amount of time to complete
	its execution then it is reffered as worst case time complexity.
	
	+ average case time complexity: when an algo takes neither minimum nor maximim
	amount of time to complete its execution then it is reffered as an average case
	time complexity.

	+ asymptotic notations:
	1. Big Oh (O) -- asymptotic upper bound -- worst case
	2. Big Omega ( ) -- asymptotic lower bound -- best case
	3. Big Theta ( ) -- asymptotic tihght bound -- average case
--------------------------------------------------------------------------------------
+ assumptions/rules in an asymtotic analysis:
	- if running time an algorithm/function contains any additive or substractive
	constant, then it can be neglected.
	 i.e. O(n+3) or O(2-n) => O(n).
	 
	- if running time of an algorithm/function has any multiplicative or divisive
	constant then it can be neglected. 
	i.e. O(2.n^2) or O(n^2/2)	=> O(n^2).

	+ O(1):
	- if any function/algo do not contains loop or call to any non-constant function
	or recursive function calls then time complexity of that function/algo is
	considered as O(1).
	e.g. swap() function
	void swap(int *ptr1, int *ptr2)
	{
		int temp = *ptr1;
		*ptr1 = *ptr2;
		*ptr2 = temp;
	}
	OR
	- if loop in a function/algo executes fixed number of times then time complexity
	is considered as O(1).
	e.g.
	for( int i = 0 ; i < c ; i++ )//where c is integer constant
	{
		//O(1) statements
	}
	
	+ O(n):
	- in a function/algo, if a loop variable either incremets or decrements by a
	constant value, then time complexity is considered as O(n).
	e.g.
	for( int i = 0 ; i < n ; i += c )//where c is integer constant
	{
		//O(1) statements
	}
	OR
	for( int i = n ; i >= 0 ; i -= c )//where c is integer constant
	{
		//O(1) statements
	}

	+ O(log n):
	- if a loop variable in an algo/function/program is either gets divided or
	multiplied then we will get time complexity in terms of log.
	e.g.
	for( int i = 0 ; i < n ; i *= c )//where c is integer constant
	{
		//O(1) statements
	}
	OR
	for( int i = 0 ; i < n ; i /= c )//where c is integer constant
	{
		//O(1) statements
	}

	+ O(n^c): for nested loops: if "c" no. of nested loops (i.e. loop inside loop)
	- in this case time complexity of an algo/function is the time taken by the statement/s
	 inside the innermost loop.
	
	for( int i = 0 ; i < n ; i++ ) //--> O(n+1) times
	{
		for( int j = 0 ; j < n ; j++ )//O( n(n+1)) times
		{
			//statements //-> n*n => n^2
		}
	}
		
	+  if any function/algo contains two consecutive loops:
	
	for( int i = 0 ; i < n ; i ++ )
	{
		//O(1) statement/s
	}
		
	for( int i = 0 ; i < m ; i ++ )
	{
		//O(1) statement/s
	}
		
	O(n) + O(m) => for m == n => O(2n) => O(n)
	
----------------------------------------------------------------------------------------
+ array:
	- "searching": to serach a key ele in a given collection/list of elements.

	1. linear search:
	Algorithm LinearSearch(A, n, key)
	{
		for( int index = 1 ; index <= SIZE ; index++ )
		{
			if( A[ index ] == key )
				return true;
		}
		return false;
	}
	
	
	* best case -- when key is found at first pos then algo does only one comparison,
	time complexity of an algo in this case = O(1).
		Big Omega(1)
		
	
	* worst case -- when either key is exists at last position or key does not exists
	algo does max "n" no. of comparisons whereas "n" size of an array, in this case
	time complexity of an algo = O(n).
	
	* average case -- if key is exists at in between position the algo takes neither
	minimum nor max time to complete its execution, in this case time complexity of
	an algo = O(n/2)

+ searching algorithms:
	1. linear search:
	- also called as "sequential search".
	- it sequentially checks each element of the list until the match is found or the
	whole list has been searched.
	- best case -- occurs when key ele is found at first position, in this case algo
	takes O(1) time.
	- worst case -- occurs when either key ele is found at last position or key ele
	does not exists, in this case algo takes O(n) time whereas n is the no. of ele's
	in the list/collection.
	- average case -- occures when key ele is exists in the list at in between
	position, in this ase algo takes O(n/2) => O(n) time.
	
	2. binary search:
	- also called as "logarithmic search" or "half interval search"
	- this algo follows "divide-and-conquer" stratergy.
	- to apply binary search prerequisite is collection/list of elements must be in a
	sorted manner.
	- in the first iteration -- mid position gets calculated and key ele gets compared
	with ele at mid position, if key ele is found then it will be the best case,
	otherwise array gets divided logically into two sub array's left subarray and
	right sub array.
	- if key ele is smaller than mid position ele then key ele gets searched into the
	left sub array only, by skipping the whole right sub array checking, or, if key ele
	is greater than mid position ele then key ele gets searched into the right sub
	array only by skipping whole left sub array.
	- the logic repeats either till key ele is not found or till size of an array is
	less than one.
	- if key ele is found at mid position in the very first iteration then no. of
	comparisons are "1" and it is considered as a best case, in this algo takes O(1)
	time, otherwise it takes O(log n) time.
	- as in every iteration this algo does 1 comparison and divides array into sub
	two arrays and key ele gets searched either one of the subarray, i.e. after every
	iteration it divides 
	
	
	=> T(n) = T(n/2) + 1
	for n = 1
	T(n) = T(1) + 1
	i.e. running time of an algo is O(1). --- trivial case
	
	for n > 1
	T(n) = T(n/2)+ 1 ..... (I)
	to get the value of T(n/2) put  n = n/2 in eq-I we get,
	=> T(n/2) = T( n/2 / 2 ) + 1
	=> T(n/2) = T(n/4) + 1 .....(II)
	
	substitute the value of T(n/2) in eq-I we get,
	=> T(n) = [ T(n/4) + 1 ] + 1
	=> T(n) = T(n/4) + 2 .....(III)
	
	
	to get the value of T(n/2) put n = n/2 in eq-II we get,
	=> T( (n/2) / 2 ) = T( (n/4) / 2 ) + 1
	=> T(n/4) = T(n/8) + 1 .... (IV)
	
	substitute the value of T(n/4) in eq-III we get,
	=> T(n/4) = [ T(n/8) + 1 ] + 2
	=> T(n/4) = T(n/8) + 3 ......(V)
	
	.
	.
	after k iterations:
	
	T(n) = T(n/2^k) + k
	
	for n = 2^k
	log n = log 2^k .... by taking log on both side
	log n = k log 2
	therefore, k = log n
	=> T(n) = T(2^k/2^k) + log n
	=> T(n) = T(1) + log n
	=> T(n) = log n
	
	and hence, T(n) = O(log n).
	
	+ difference between linear search and binary search:
	- in linear search after every iteration search space is reduced by 1 i.e. (n-1)
	and in binary search search space is reduced by half of elements i.e. by (n/2).
	- worst case time complexity of linear search is O(n) and of binary search is
	O(log n) hence binary search is efficient than linear search.
	- binary search cannot be applied on linked list.

